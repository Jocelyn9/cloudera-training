<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
    <head>
        <title>Introduction to Pig</title>
	    <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">	    
    </head>

    <body>
	    <table class="pagecontent" border="0" cellpadding="0" cellspacing="0" width="100%" bgcolor="#ffffff">
		    <tr>
			    <td valign="top" class="pagebody">
				    <div class="pageheader">
					    <span class="pagetitle">
                            Workshop Exercise: Introduction To Pig
                                                    </span>
				    </div>

<p>This project is designed to familiarize you with the <b>Pig</b> data processing
language. In a previous exercise, we <a href="SqoopExercise.html">imported data from
a database into Hive</a>. The data is stored, however, in HDFS (in the
<tt>/user/hive/warehouse</tt> directory), so we can work with it using other systems
like Pig too.</p>

<p>Much of this exercise is drawn from the lecture component. You may <a
href="../IntroToPig.pdf">review the slides for this</a> if you need to.</p> 

<p>You may also find the 
<a href="http://hadoop.apache.org/pig">Pig website</a> and
<a href="http://wiki.apache.org/pig">wiki</a> useful references.</p>

<hr/>
<h2>Getting Started</h2>

<p>To log in to Grunt, the Pig shell, run <tt>pig -x local</tt> at the terminal. You should see
something like the following:
</p>

<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
$ cd git/data
$ pig -x local
grunt&gt;
</pre>
</div></div>

<p>This will tell Pig to execute without connecting to HDFS, just the local filesystem.
We're going to use this for the first exercise. Make sure you switch to the <tt>~/git/data</tt>
directory, where a data file you'll need is present.</p>

<h3>Aggregation</h3>

<p>We can perform aggregation in Pig; e.g., counting the number of times users
appear in the Excite data set:</p>

<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
grunt&gt; log = LOAD 'excite-small.log' AS (user, timestamp, query);
grunt&gt; grpd = GROUP log BY user;
grunt&gt; cntd = FOREACH grpd GENERATE group, COUNT(log);
grunt&gt; STORE cntd INTO 'output';
</pre>
</div></div>

<p>To view the output of this operation, open another shell, change to the same directory
as you launched Pig from, and <tt>cat</tt> or <tt>head</tt> the output file:</p>


<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
$ cd ~/git/data
$ head output 
</pre>
</div></div>

<p>Note that commands are evaluated "lazily;" only when you run the <tt>STORE</tt> command
which forces output to appear, is the full script executed. Without storing results,
no MapReduce passes will run.</p>

<h3>Filtering</h3>

<p>In addition to performing aggregates, we can also filter the records we process, much
as a <tt>WHERE</tt> clause does in SQL or Hive:</p>

<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
grunt&gt; log = LOAD 'excite-small.log' AS (user, timestamp, query);
grunt&gt; grpd = GROUP log BY user;
grunt&gt; cntd = FOREACH grpd GENERATE group, COUNT(log) AS cnt;
grunt&gt; fltrd = FILTER cntd BY cnt &gt; 50;
grunt&gt; STORE fltrd INTO 'output';
</pre>
</div></div>

<h3>Sorting</h3>

<p>And finally, the results can be ordered:</p>

<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
grunt&gt; log = LOAD 'excite-small.log' AS (user, timestamp, query);
grunt&gt; grpd = GROUP log BY user;
grunt&gt; cntd = FOREACH grpd GENERATE group, COUNT(log) AS cnt;
grunt&gt; fltrd = FILTER cntd BY cnt &gt; 50;
grunt&gt; srtd = ORDER fltrd BY cnt;
grunt&gt; STORE srtd INTO 'output';
</pre>
</div></div>



<hr/>
<h2>Connect Pig to Hadoop</h2>

<p>In addition to running against the local filesystem, you can run Pig in conjunction
with HDFS and a MapReduce cluster (though inside the virtual machine, this cluster has
only one node--itself). Running <tt>pig</tt> with no arguments will connect to the
Hadoop cluster identified in its configuration file:</p>

<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
$ pig
2009-05-28 18:30:57,466 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine
- Connecting to hadoop file system at: hdfs://localhost:8020
2009-05-28 18:30:58,248 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine
- Connecting to map-reduce job tracker at: localhost:8021
grunt&gt;
</pre>
</div></div>



<hr>
<h2>Examining the Data</h2>

<p>You should have imported two tables of information from MySQL into HDFS. Each table is
in a directory under <tt>/user/hive/warehouse</tt>. You can see the directories by running:


<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
$ hadoop fs -ls /user/hive/warehouse
</pre>
</div></div>

<p>If the <tt>shake_freq</tt> and <tt>bible_freq</tt> tables aren't there, you should
run through the <a href="SqoopExercise.html">Sqoop exercise</a> now.</p>

<p>You can see that each table is represented simply by a set of files:</p>
<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
$ hadoop fs -ls /user/hive/warehouse/shake_freq
</pre>
</div></div>

<p>Now let's run a Pig job to see what words are in both datasets:

<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
bard   = LOAD '/user/hive/warehouse/shake_freq' USING PigStorage('\t') AS (freq, word);
kjv    = LOAD '/user/hive/warehouse/bible_freq' USING PigStorage('\t') AS (freq, word);
inboth = JOIN bard BY word, kjv BY word;
STORE inboth INTO 'output';
</pre>
</div></div>

<p>Don't forget now that instead of looking for results in a local file named <tt>output</tt>,
they'll be written to a directory in HDFS named <tt>output</tt>, so you'll need to use <tt>hadoop fs
-ls</tt>, <tt>hadoop fs -cat</tt>, etc. to view your results.</p>

<p>This query will find the words that are in the Bible and not in Shakespeare:
</p>


<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
bard   = LOAD '/user/hive/warehouse/shake_freq' USING PigStorage('\t') AS (freq, word);
kjv    = LOAD '/user/hive/warehouse/bible_freq' USING PigStorage('\t') AS (freq, word);
grpd   = COGROUP bard BY word, kjv BY word;
nobard = FILTER grpd BY COUNT(bard) == 0;
out    = FOREACH nobard GENERATE FLATTEN(kjv);
STORE out INTO 'output';
</pre>
</div></div>


<hr>
<h2>Extension 1: Single-source high-frequency words</h2>

<ul>
<li>Write a script that determines the highest-frequency word from the Shakespeare data set
that does not appear in the Bible dataset.</li>
</ul>

<hr>
<h2>Extension 2: Proper Nouns</h2>

<p>Goal: identify all the proper nouns used in Shakespeare's works.</p>

<p>You'll need to write a Pig script, naturally. :) To help you, we've written a preprocessing
script. You'll need to run it to generate an intermediate dataset in HDFS. This script
creates an index from words to sentences. For each sentence a word appears in, it will
emit a record of the form <tt>word (tab) sentence_id</tt>. For example, the input
<tt>One sentence. Another sentence.</tt> will yield the following output:
</p>

<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
Another  Another 2050940439
One One 1502083882
sentence  Another 2050940439
sentence  One 1502083882
</pre>
</div></div>

<p>This script requires having the Shakespeare text loaded into HDFS. If you have not
already completed the <a href="GettingFamiliar.html">Getting Familiar With Hadoop</a>
exercise, please follow the instructions in the 
<a href="GettingFamiliar.html#GettingFamiliar-UploadingFiles">Uploading Files</a>
section of the exercise to load the Shakespeare dataset into HDFS.</p>


<p>The preprocessing script is based on the inverted index code, and is written in python.
To run the script, execute:</p>

<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
$ cd ~/git/exercises/shakespeare/sentence-idx
$ ./run-indexer input
</pre>
</div></div>

<p>This assumes that your Shakespeare data set has been loaded into the <tt>input</tt>
directory in HDFS. It will create its output in a directory called <tt>input_idx</tt>.
</p>

<p><b>Hint: regular expressions</b>. We can approximate proper nouns as being words
which are capitalized, but not at the start of a sentence. Recall that the <tt>sentence_id</tt>
contains the first word of the sentence. (Note: due to the text we're
using, this is actually going to generate a lot of false positives--but this serves
for exercise purposes.) Finding the correct words
can be done by using a <tt>FILTER</tt> statement with a regular expression.
See the <a href="../IntroToPig.pdf">Intro to Pig slides</a> for hints on using
regular expressions and string matching syntax in Pig.</p>

<p>You'll need to do something more involved to determine whether one string starts
with another. Regular expressions must be constant strings, and can't be data dependent.
Instead, you need to use a <i>user defined function</i> (UDF) to determine whether 
one string starts with another. We've written a <tt>StartsWith()</tt> UDF for you, 
in Java. You'll need to compile the jar that contains this. To do so, run:
</p>

<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
$ cd ~/git/exercises/intro-to-pig/udf/
$ ant
</pre>
</div></div>

<p>This will emit a file named <tt>textudf.jar</tt>. Then in your pig script, the command
<tt>REGISTER /path/to/textudf.jar;</tt> will load the jar file into your environment.
You can then use the <tt>StartsWith()</tt> method that we wrote. Its signature is:</p>

<div class="code panel" style="border-width: 1px;"><div class="codeContent panelContent">
<pre class="code-java">
/** Returns true if haystack.startsWith(needle) */
boolean StartsWith(String haystack, String needle);
</pre>
</div></div>

<p>If you're feeling ambitious, you could try to reimplement the UDF (or a similar one)
yourself; see the
<a href="http://hadoop.apache.org/pig/docs/r0.2.0/udf.html">UDF Manual</a> in
the Pig documentation for more details as to how.</p>

<hr/>
<h2>Solutions</h2>

<p>When you're done and want to check your work (or if you get stuck and need a hint),
solutions to both extensions are in the
<tt>~/git/exercises/intro-to-pig/solution</tt> directory.</p>

</body>
</html>
