<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html lang="en"><head>


<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="description" content="Module 6: Related Topics">
<title>Hadoop Tutorial</title>

<link rel="stylesheet" type="text/css" href="resource/combo.css">
<script src="resource/fpc.js" type="text/javascript" defer="defer"></script></head><body style="margin: 0pt;"><iframe style="border-width: 0pt; position: absolute; visibility: visible; width: 10em; height: 10em; top: -134px; left: -134px;" title="Text Resize Monitor" id="_yuiResizeMonitor"></iframe>
<div id="clearfooter" style="padding-top: 20px;">
<!-- used to make room for the #footer -->
</div>
<div id="bodycontainer">

<!--[if IE 6]>

<style type="text/css" media="screen">
#topAreaR p{margin:0;padding:0;}

#headeraL{float:left;color:#9f9f9f;font-size:85%;}
#searchFld{height:1.46em;border:1px solid #cecece;}
#searchFld{margin:0;}
#sub{position:relative;left:2px;}

#headeraL{height:4em;}

#subHIntCont{position:relative;top:0em;}

#ydnNav{float:left;padding:0;top:1em;}
.yuimenubaritemlabel{padding:0;}

#headerSubSandbag {top:5.9em;}

h4.fullHorange{width:98%;margin:0;}
#recBlPst{width:18em;}
#recBlPst3{width:17em;}
#upcomingEventsBoxCont{width:17.7em;float:right;padding:0;}
#upcomingEventsBox{float:right;margin:0}
#upcomingEventsBox .blogRackTitle{margin-left:.3em;width:15em}
#upcomingEventsBox .blogRackDate{margin-left:.67em;}

#topFeaturedTabs ul li strong strong{background-color:#9dc1d6;}
#topFeaturedTabs ul li strong strong strong{background-color:#9dc1d6;}
#topFeaturedTabs ul li a:hover strong strong{background-color:#6fa4c3;}
#topFeaturedTabs ul li a:hover strong strong strong{background-color:#6fa4c3;}

.prefTabs{color:#aaa;float:left;position:relative;margin:.5em .8em 0em 2em;font-weight:bold;}
#featureBox1, #featureBox2, #featureBox3, #featureBox4{border:1px solid #d9d9d9;}
.innerSubMenuArea{border:1px solid #d9d9d9;}

#breadcrumbs{position:relative;top:1.8em;}

#getAppBox{border:1px solid #D9D9D9;}

.recentPat{margin:0;margin-left:.3em;}
#recentPatternsContainer {clear:both;float:left;}

html body #doc4.yui-t7 #bd .yui-g .yui-gc div.first, .yui-gc div.first, .yui-gd .yui-g, .yui-gd .yui-u{width:67%;}
html body #doc4.yui-t7 #bd .yui-g .yui-gb .yui-u, .yui-gb .yui-g, .yui-gb .yui-gb, .yui-gb .yui-gc, .yui-gb .yui-gd, .yui-gb .yui-ge, .yui-gb .yui-gf, .yui-gb .yui-u, .yui-gc .yui-u, .yui-gc .yui-g, .yui-gd .yui-u{width:29%;}
html body #doc4.yui-t7 #bd .yui-g .yui-gb .yui-u, .yui-gb .yui-g, .yui-gb .yui-gb, .yui-gb .yui-gc, .yui-gb .yui-gd, .yui-gb .yui-ge, .yui-gb .yui-gf, .yui-gb .yui-u, .yui-gc .yui-u, .yui-gc .yui-g, .yui-gd .yui-u{margin-left:.5%;}

</style>

<![endif]-->
<!--[if IE 7]>

<style type="text/css" media="screen">
#subHIntCont{position:relative;top:0;}
#ydnNav{float:left;padding:.1em 0;position:relative;left:0;}
#ydnNav .yuimenu .colGroup { margin-right: 1.5em;  }
#breadcrumbs{position:relative;top:1.8em;}
</style>

<![endif]-->


<div id="mainCMaxCont">
    <div id="mainCContainer">
        <div id="mainCIntCont">
            <div id="internalContainer">
                <div id="doc4" class="yui-t6">
                    <div id="bd">
                        <div id="yui-main">
                        <div class="yui-b">
<h2>Module 6: Related Topics</h2>

<div style="text-align: center;">
<p> <a href="module5.html">Previous module</a> &nbsp;|&nbsp; <a href="index.html">Table of contents</a> &nbsp;|&nbsp; <a href="module7.html">Next module</a></p>
</div>

<a name="intro"></a>
<h3>Introduction</h3>
 
<p>Hadoop by itself allows you to store and process
very large volumes of data. However, building a large-scale distributed
system can require functionality not provided by this base. Several
other tools and systems have been created to fill the gaps and deliver
a more full-featured set of distributed systems engineering tools.</p>
  
 <a name="goals"></a>
<h3>Goals for this Module:</h3>
 
<ul>

   
  <li class="bullist">Understand how distributed consensus systems can
be used to bootstrap larger distributed systems.</li>
   
  <li class="bullist">Understand how to write queries in the Pig
log-processing language</li>
   
</ul>
  
<a name="outline"></a>
<h3>Outline</h3>
 
<ol>
   
  <li><a href="#intro">Introduction</a></li>
  <li><a href="#goals">Goals for this Module</a></li>
  <li><a href="#outline">Outline</a></li>

  <li><a href="#zookeeper">ZooKeeper</a></li>
  <ol>
    <li><a href="#zoomotivation">Motivation</a></li>
    <li><a href="#datastore">Data Storage in ZooKeeper</a></li>
    <li><a href="#zoouses">ZooKeeper Applications</a></li>
    <li><a href="#consensus">Distributed Consensus</a></li>

  </ol>
  <li><a href="#pig">Pig</a></li>
  <ol>
    <li><a href="#pigmotive">Motivation</a></li>
    <li><a href="#piglatin">Pig Latin</a></li>
    <li><a href="#datatypes">Pig Latin Data Types</a></li>
    <li><a href="#dataload">Loading Data Into Pig</a></li>

    <li><a href="#operators">Pig Latin Operators</a></li>
    <li><a href="#pigsetup">Setting Up Pig</a></li>
  </ol>
  <li><a href="#refs">References</a></li>
</ol>

<a name="zookeeper"></a>
<h3>ZooKeeper</h3>
  
<a name="zoomotivation"></a>
<h4 class="sectionSubH">Motivation</h4>
 
<p>Suppose you are building a large-scale
distributed system. Several different services need to be brought
online and must discover one another. It is not guaranteed that each
service will have a fixed master IP address. For example, it may be the
case that you start the same service on 100 nodes, and they elect a
master from among whichever of the 100 boots first. Each of these
disparate services must communicate with each other. How do all the
nodes of each service find the master IP address of each other service?
How do all the nodes in a single service agree on which one of them
becomes the master? </p>
 
<p>ZooKeeper is a service designed to handle
all of these problems. ZooKeeper will allow you to store small amounts
of information in a central location. It provides coordinated access to
this information. Most importantly, it provides high-availability: the
ZooKeeper service is intended to run on a set of several machines,
which prevents the loss of individual nodes from bringing down the
cluster. But these nodes communicate information in a careful way,
ensuring that all nodes in the ZooKeeper cluster provide the same
consistent answer for a query, regardless of which ZooKeeper server you
contact.</p>

<a name="datastore"></a>
<h4 class="sectionSubH">Data Storage in ZooKeeper</h4>
 

<p>Several ZooKeeper daemons can be started on
different machines. Clients can connect to any daemon in the cluster;
the clients will always see the same view of the ZooKeeper world
regardless of which daemon they connect to. User data is stored in
objects with a hierarchical addressing system similar to that used by a
conventional file system. It has a root object named <tt>/</tt>, and
additional nodes can be extended off of this in a tree-like fashion.
Each node of the tree can both hold data (i.e., act like a file) and
have child nodes (i.e., act like a directory). The amount of data that
can be stored in an object is small: there is a hard cap of 1 MB. The
reason for the cap is so that the entire data store can be stored in
the RAM of the ZooKeeper machines. This allows requests to be
dispatched with high throughput. Changes are written to disk to provide
permanence, but read requests are entirely handled by the data cached
in memory. This is usually not a major limitation; the data stored at a
node is intended to be used as a pointer. For example, ZooKeeper may
know about the filename in another conventional file system, which
contains the authoritative configuration file for a distributed system.
The distributed system components first contact ZooKeeper to get the
definitive filename, and then fetch that file for the configuration. </p>

<a name="zoouses"></a>
<h4 class="sectionSubH">ZooKeeper Applications</h4>
 
<p>ZooKeeper can be used for a variety of distributed
coordination tasks. In addition to leader election, system
bootstrapping, and various types of locks (mutual exclusion,
reader/writer, etc), other synchronization primitives such as barriers,
producer/consumer queues, priority queues, and multi-phase commit
operations can be encoded in ZooKeeper. The ZooKeeper <a href="http://wiki.apache.org/hadoop/ZooKeeper/Tutorial">tutorial</a>
and <a href="http://wiki.apache.org/hadoop/ZooKeeper/ZooKeeperRecipes">recipes</a>

pages describe how to implement these algorithms. ZooKeeper itself is
implemented in Java, but provides APIs for both Java- and C-based
programs.</p>
<p>ZooKeeper can also be used as a central message board for an
application. Individual nodes of a distributed system can store their
current operational status in ZooKeeper for easy central reporting. The
ZooKeeper service can also be used to form sub-groups of nodes or other
hierarchical arrangements within a distributed system. </p>
<p>As mentioned, data stored in ZooKeeper is accessed by manipulating
the nodes in the data hierarchy. This is done in a manner similar to
file system access. But ZooKeeper does not implement the POSIX file
system API. On the other hand, it also adds a set of other primitives
not ordinarily found in a file system. Nodes can be opened with a
number of special flags. One such flag is "ephemeral," meaning that the
node disappears when the client who opened it disconnects. Another such
flag is "sequence," which means that ZooKeeper will append a sequential
id number to the node name you are trying to create. These id numbers
are handed out in order, and the same id number is not reused.
ZooKeeper does not provide exclusive locks on nodes directly, but a
lock can be created by careful use of the ephemeral and sequence flags.
The <a href="http://wiki.apache.org/hadoop/ZooKeeper/ZooKeeperRecipes">ZooKeeper</a>
recipes wiki page describes how to implement global locks using
these flags. It also describes protocols for implementing shared
(reader-writer) and revocable locks. </p>

<a name="consensus"></a>
<h4 class="sectionSubH">Distributed Consensus</h4>
 
<p>A reasonable question is how the ZooKeeper
service can function across multiple nodes and remain synchronized. If
distributed synchronization is why your services must use ZooKeeper,
how does ZooKeeper itself bootstrap this capability?</p>

 
<p>ZooKeeper implements a distributed consensus
protocol. ZooKeeper internally uses a leader election protocol such as
Paxos to determine which node in the ZooKeeper service is the master.
While clients connect to any node in the ZooKeeper service, these
additional nodes will forward <i>agreed-upon facts</i> back to
clients. Updates to the shared state require the intervention of the
master. All updates to the shared state are ordered with timestamps.
These timestamped updates are then disseminated to the nodes in the
ZooKeeper service. When a majority of nodes acknowledge an update, it
is said to be held by a <i>quorum</i> of the nodes. Any fact that a
quorum has agreed upon may be returned to clients. Conversely, any
updates that have not reached a quorum will not be returned to the
clients. The timestamps are used to order the updates to elements of
the data store. If multiple updates are made to the state of an
individual node, the newest update is used.</p>
 
<p>The use of a quorum ensures that the service
always returns consistent answers. Because a vote is effectively held
before returning a response, any nodes which hold stale data will be
outvoted by the nodes with more current information. This also makes
ZooKeeper resilient to failure. Up to 49% of the ZooKeeper service
nodes can shut down or become desynchronized before ZooKeeper loses its
ability to authoritatively answer responses. So if 11 nodes run
ZooKeeper, up to 5 of these may disconnect without incident. After more
than half the nodes fail, ZooKeeper will refuse service until the
machines are restored. </p>
 
<p>If the node of the ZooKeeper cluster which was
elected leader fails, then a new leader election will be held and the
cluster will continue to function.</p>

 
<p>The reason for electing a leader in such a
system is to ensure that timestamps assigned to updates are only issued
by a single authority. ZooKeeper is designed to reduce or eliminate
possible race conditions in distributed applications.</p>
 
<p>One consequence of ZooKeeper's design is that it
is intended to serve many more read requests than writes. A ZooKeeper
cluster can handle hundreds or thousands of clients, issuing many tens
of thousands of requests per second--if the majority of these requests
(90--99%) are reads. Only a small fraction should be updates.</p>

<a name="zooexample"> </a>
<h3>ZooKeeper Example</h3>
 
<p>The following code excerpt shows how to use
ZooKeeper to implement a "barrier." A barrier separates a process into
two logical halves. Multiple machines running in coordination with one
another will all perform the first half of the process. No machine can
begin the second half of the process until everyone has completed the
first half. The barrier sits between these processes. As nodes reach
the barrier, they all wait until everyone has reached the barrier. Then
all nodes are released to begin the second half. A distributed barrier
implementation written for ZooKeeper follows: </p>

 
<div class="code"><pre class="codeblue">Watcher watcher = new Watcher() {<br>  public void process(WatchEvent event) {}<br>};<br><br>ZooKeeper zk = new ZooKeeper(hosts, 3000, watcher);<br><br>Object notifyObject = new Object();<br>String root;<br>int size;<br><br>Barrier(ZooKeeper zk, String name, int size) throws KeeperException, InterruptedException {<br>  this.zk = zk;<br>  this.root = name;<br>  this.size = size;<br>  // Make sure the  barrier node exists<br>  try {<br>    zk.create(root, new byte[0], Ids.OPEN_ACL_UNSAFE, 0);<br>  } catch (NodeExistsException e) {}<br>}<br><br><br>b.enter()<br>/** work with everyone **/<br>b.leave()<br><br><br><br>/**<br> * Join barrier<br> * @return<br> * @throws KeeperException<br> * @throws InterruptedException */<br>boolean enter() throws KeeperException, InterruptedException {<br>  zk.create(root + "/" + name, new byte[0], Ids.OPEN_ACL_UNSAFE, CreateFlags.EPHEMERAL);<br>  while (true) {<br>    synchronized (notifyObject) {<br>      ArrayList&lt;String&gt; list = zk.getChildren(root, new Watcher() {<br>        public void process(Event e) { notifyObject.notifyAll(); }<br>      });<br><br>      if (list.size() &lt; size)  {<br>        notifyObject.wait();<br>      } else {<br>        return true;<br>      }<br>    }<br>  }<br>}<br><br>/**<br> * Wait until all reach barrier<br> * @return<br> * @throws KeeperException<br> * @throws InterruptedException */<br>boolean leave() throws KeeperException, InterruptedException {<br>  zk.delete(root + "/" + name, 0);<br>  while (true) {<br>    synchronized (notifyObject) {<br>      ArrayList&lt;String&gt; list = zk.getChildren(root, new Watcher() {<br>        public void process(Event e) { notifyObject.notifyAll(); }<br>      });<br><br>      if (list.size() &gt; 0) {<br>        notifyObject.wait();<br>      } else {<br>        return true;<br>      }<br>    }<br>  }<br>}</pre>

</div>
 
<p><i>Listing 6.1: ZooKeeper Barrier Example</i></p>

<a name="pig"></a>
<h3>Pig</h3>

<a name="pigmotive"></a>
<h4 class="sectionSubH">Motivation</h4>
<p>Pig is a platform for analyzing large data sets.
Pig's language, Pig
Latin, lets you specify a sequence of data transformations such as
merging data sets, filtering them, and applying functions to records or
groups of records. Users can create their own functions to do
special-purpose processing.</p>

<p>Pig Latin programs execute in a distributed
fashion on a cluster. Our
current implementation compiles Pig Latin programs into Map/Reduce
jobs,
and executes them using Hadoop on Kryptonite.</p>
 
<p>Thur purpose of Pig is to answer queries over
semi-structured data such as log files. Large volumes of data are in
mostly-organized formats such as log files, which define a set of
standard fields for each entry. While the MapReduce programming model
on top of Hadoop provides a convenient mechanism for delivering a large
volume of log-structured information to an analysis program, writing
analyses of mostly-structured information involves writing a large
amount of tedious processing code.</p>
 
<p>Pig is a high-level language for writing queries
over this sort of data. A query planner then compiles queries written
in this language (called "Pig Latin") into maps and reduces which are
then executed on a Hadoop cluster.</p>
 
<p>Anything which could be written in Pig can also
be implemented as straight Java-based Hadoop MapReduce. But while
individual programmers could develop their own suite of data analysis
functions such as <i>join</i>, <i>order by</i>, etc., this requires
individual programmers to develop their own (non-standard) libraries,
and test them. Pig provides a tested and supported suite of the most
common data-aggregation functions. It also allows programmers to
provide their own application-specific code for purposes of loading and
saving data, as well as for performing more complicated
record-by-record evaluations.</p>

<a name="piglatin"></a>
<h4 class="sectionSubH">Pig Latin</h4>
 
<p>The programming language used to write Pig
queries is called <i>Pig Latin</i>.</p>
 
<p>The MapReduce programming model can be thought of
as composed of three distinct phases:</p>
 

<ol>
   
  <li>Process input records</li>
   
  <li>Form groups of related records</li>
   
  <li>Process groups into outputs</li>

   
</ol>
 
<p>In MapReduce, the first two of these steps are
handled by the mapper, and the third step is handled by the reducer.
Pig Latin exposes explicit primitives that perform actions from each
phase. These primitives can be composed and reordered. Furthermore, it
includes additional primitives to handle operations such as filtering
and joining data sets.</p>

<a name="datatypes"></a>
<h4 class="sectionSubH">Pig Latin Data Types</h4>
 
<p>Values in Pig Latin can be expressed by four
basic data types: </p>

 
<ul>
   
  <li class="bullist">An <b>atom</b> is any atomic value (e.g., <tt>"fish"</tt>)</li>
   

  <li class="bullist">A <b>tuple</b> is a record of multiple
values with fixed arity. e.g., <tt>("dog", "sparky")</tt>.</li>
   
  <li class="bullist">A <b>data bag</b> is a collection of an
arbitrary number of values. e.g., <tt>{("dog", "sparky"), ("fish",
"goldie")}</tt>. Data bags support a <i>scan</i> operation for
iterating through their contents.</li>

   
  <li class="bullist">A <b>data map</b> is a collection with a
lookup function translating keys to values. e.g., <tt>["age" : 25]</tt></li>
   
</ul>
 
<p>All data types are fully nestable; bags may
contain tuples, and maps may contain bags or other maps, etc. This
differs from a traditional database model, where data must be
normalized into lists of atoms. By allowing data types to be composed
in this manner, Pig queries line up better to the conceptual model of
the data held by the programmer. Data types may also be heterogeneous.
For example, the fields of a tuple may each have different types; some
may be atoms, others may be more tuples, etc. The values in a bag may
hold different types, as may the values in data maps. These can vary
from one record to the next in the bag. Data map keys must be atoms,
for efficiency reasons.</p>

<a name="dataload"></a>
<h4 class="sectionSubH">Loading Data Into Pig</h4>
 
<p>The first step in using Pig is to load data into
a program. Pig provides a <tt>LOAD</tt> statement for this purpose.
Its format is: <tt><i>result</i> = LOAD '<i>filename</i>' USING <i>fn</i>()
AS (<i>field1</i>, <i>field2</i>, ...)</tt>.</p>

 
<p>This statement returns a bag of values of all the
data contained in the named file. Each record in the bag is a tuple,
with the fields named by <i>field1</i>, <i>field2</i>, etc. The <i>fn</i>()
is a user-provided function that reads in the data. Pig supports
user-provided Java code throughout to handle the application-specific
bits of parsing. Pig Latin itself is the "glue" that then holds these
application-specific functions together, routing records and other data
between them.</p>
 
<p>An example data loading command (taken from <a href="http://www.cs.cmu.edu/%7Eolston/publications/sigmod08.pdf">this
paper on Pig</a>) is:</p>
<div class="code">

<pre class="codeblue">queries = LOAD 'query_log.txt'<br>          USING myLoad()<br>          AS (userId, queryString, timestamp)</pre>
</div>
<p>The user-defined functions to load data (e.g., <i>myLoad()</i>) do
not need to be provided. A default function for loading data exists,
which will parse tab-delimited records. If the programmer did not
specify field names in the <tt>AS</tt> clause, they would be addressed
by positional parameters: <tt>$0</tt>, <tt>$1</tt>, and so forth.</p>

<p>The default loader is called <i>PigStorage()</i>. This loader can
read files containing character-delimited tuple records. These tuples
must contain only atomic values; e.g., <tt>cat, turtle, fish</tt>.
Other loaders are listed in the <a href="http://wiki.apache.org/pig/PigBuiltins">PigBuiltins</a> page of
the Pig wiki. <tt>PigStorage()</tt> takes as an argument the character
to use to delimit fields. For example, to load a table of three
tab-delimited fields, the following statement can be used:</p>
<div class="code"><pre class="codeblue">data = LOAD 'tab_delim_data.txt' USING PigStorage('\t') AS (user, time, query)</pre>
</div>
<p>A different argument could be passed to <tt>PigStorage()</tt> to
read comma- or space-delimited fields.</p>

<a name="operators"></a>
<h4 class="sectionSubH">Pig Latin Operators</h4>
 
<p>Pig Latin provides a number of operators which
filter, join, or otherwise organize data.</p>
 
<p><b>FOREACH:</b> The <tt>FOREACH</tt> command
operates on each element of a data bag. This is useful, for instance,
for processing each input record in a bag returned by a <tt>LOAD</tt>

statement. </p>
<div class="code"><pre class="codeblue">FOREACH <i>bagname</i> GENERATE <i>expression</i>, <i>expression</i>...</pre>
</div>
 
<p>This statement iterates over the contents of a
bag. It applies the expressions on the right of the <tt>GENERATE</tt>

keyword to the data provided by the current record emitted from the
bag. The expressions may be, for example, the names of fields. So to
extract the names of all users who accessed the site (based on the <tt>query_log.txt</tt>
example shown above), we could write a query like: </p>
 
<div class="code"><pre class="codeblue">FOREACH queries GENERATE userId;</pre>
</div>
 
<p>In the <tt>FOREACH</tt> statement, each element
of the bag is considered independently. There are no expressions which
reference multiple elements being extracted from the bag's iterator at
a time; this allows the statement to be processed in parallel using
Hadoop MapReduce.</p>

 
<p>Expressions emitted by the <tt>GENERATE</tt>
element are not limited to the names of fields; they can be fields (by
name like <tt>userId</tt> or by position like <tt>$0</tt>),
constants, algebraic operations, map lookups, conditional expressions,
or <tt>FLATTEN</tt> expressions, described below.</p>
 

<p>Finally, these expressions may also call
user-provided functions that are written in Java. These user-provided
functions have access to the entire current record through a Pig
library; in this way, Pig can be used as the heavy-lifting component to
automate record-by-record mapping using an application-specific Java
function to perform tricky parsing or evaluation logic. Pig also
provides several of the most commonly-needed functions, such as <tt>COUNT</tt>,
<tt>AVG</tt>, <tt>MIN</tt>, <tt>MAX</tt>, and <tt>SUM</tt>.</p>
 
<p><b>FLATTEN</b> is an expression which will
eliminate a level of nesting. Given a tuple which contains a bag, <tt>FLATTEN</tt>

will emit several tuples each of which contains one record from the
bag. For example, if we had a bag of records containing a person's name
and a list of types of pets they own:</p>
 
<div class="code"><pre class="codeblue">(Alice, { turtle, goldfish, cat })<br>(Bob, { dog, cat })</pre>
</div>
 
<p>A <tt>FLATTEN</tt> command would eliminate the
inner bags like so:</p>
 

<div class="code"><pre class="codeblue">(Alice, turtle)<br>(Alice, goldfish)<br>(Alice, cat)<br>(Bob, dog)<br>(Bob, cat)</pre>
</div>
 
<p><b>FILTER</b> statements iterate over a bag and
return a new bag containing all elements which pass a conditional
expression, e.g.: </p>
 
<div class="code">

<pre class="codeblue">adults = FILTER people BY age &gt; 21;</pre>
</div>
 
<p>The <b>COGROUP</b> and <b>JOIN</b> operations
perform similar functions: they unite related data elements from
multiple data sets. The difference is that <tt>JOIN</tt> acts like the
SQL JOIN statement, creating a flat set of output records containing
the joined cross-product of the input records. The <tt>COGROUP</tt>

operator, on the other hand, groups the elements by their common field
and returns a set of records each containing two separate bags. The
first bag is the records of the first data set with the common field,
and the second bag is the records of the second data set containing the
common field. </p>
 
<p>To illustrate the difference, suppose we had the
flattened data set mapping people to their pets, and another flattened
data set mapping people to their friends. We could create a "pets of
friends" data set out of these like the following. Here are the input
data sets:</p>
 
<div class="code"><pre class="codeblue">pets: (owner, pet)<br>----------------------<br>(Alice, turtle)<br>(Alice, goldfish)<br>(Alice, cat)<br>(Bob, dog)<br>(Bob, cat)<br><br>friends: (friend1, friend2)<br>----------------------<br>(Cindy, Alice)<br>(Mark, Alice)<br>(Paul, Bob)</pre>

</div>
 
<p>Here is what is returned by <tt>COGROUP</tt>:</p>
 
<div class="code"><pre class="codeblue">COGROUP pets BY owner, friends BY friend2; <i>returns:</i>

( Alice, {(Alice, turtle), (Alice, goldfish), (Alice, cat)},
         {(Cindy, Alice), (Mark, Alice)} )
( Bob, {(Bob, dog), (Bob, cat)}, {(Paul, Bob)} )</pre>
</div>

 
<p>Contrasted with the more familiar,
non-hierarchical <tt>JOIN</tt> operator:</p>
 
<div class="code"><pre class="codeblue">JOIN pets BY owner, friends BY friend2; <i>returns:</i>

(Alice, turtle, Cindy)
(Alice, turtle, Mark)
(Alice, goldfish, Cindy)
(Alice, goldfish, Mark)
(Alice, cat, Cindy)
(Alice, cat, Mark)
(Bob, dog, Paul)
(Bob, cat, Paul)</pre>
</div>

 
<p>In general, <tt>COGROUP</tt> command supports
grouping on as many data sets as are desired. Three or more data sets
can be joined in this fashion. It is also possible to group up elements
of only a single data set; this is supported through an alternate
keyword, <b>GROUP</b>.</p>
 
<p>A <tt>GROUP ... BY</tt> statement will organize
a bag of records into bags of related items based on the field
identified as their common key field. e.g., the <i>pets</i> bag from
the previous example could be grouped up with: </p>

 
<div class="code"><pre class="codeblue">GROUP pets BY owner; <i>returns:</i>

( Alice, {(Alice, turtle), (Alice, goldfish), (Alice, cat)} )
( Bob, {(Bob, dog), (Bob, cat)} )</pre>
</div>
 
<p>In this way, <tt>GROUP</tt> and <tt>FLATTEN</tt>

are effectively inverses of one another.</p>
 
<p>More complicated statements can be realized as
well: operations which expect a data set as input do not need to use an
explicitly-named data set; they can use one generated "inline" with
another <tt>FILTER</tt>, <tt>GROUP</tt> or other statement.</p>
 
<p>When the final data set has been created by a
Pig Latin script, the output can be saved to a file with the <b>STORE</b>
command, which follows the form:</p>

 
<div class="code"><pre class="codeblue">STORE <i>data set</i> INTO '<i>filename</i>' USING <i>function</i>()</pre>
</div>
 
<p>The provided function specifies how to serialize
the data to the file; if it is omitted, then a default serializer will
write plain-text tab-delimited files.</p>

 
<p>A number of additional operators exist for the
purposes of removing duplicate records, sorting records, etc. <a href="http://www.cs.cmu.edu/%7Eolston/publications/sigmod08.pdf">This
paper</a> explains the additional operators and expression syntaxes in
greater detail.</p>

<a name="pigsetup"></a>
<h4 class="sectionSubH">Setting Up Pig</h4>
 
<p>Pig is an Apache incubator project; it has not
made official packaged releases, but the source code can be retrieved
from their <a href="http://subversion.tigris.org/">subversion</a>
repository. </p>

<p>The <a href="http://incubator.apache.org/pig/">Pig Incubator</a>
home page contains instructions on retrieving the Pig sources and
compiling them.</p>

<a name="refs"></a>
<h3>References</h3>
<a href="pigtutorial.html">Pig Tutorial</a>: Included in this package
for user to get hands-on help<br>
<p>Olston, C., Reed, B., Srivastava, U., <i>et al</i>. <a href="http://www.cs.cmu.edu/%7Eolston/publications/sigmod08.pdf">Pig
Latin</a>: A Not-So-Foreign Language for Data Processing. In
Proceedings of the ACM SIGMOD 2008 International Conference on
Management of Data. Vancouver, Canada, June 2008.</p>

<p><a href="http://hadoop.apache.org/pig/">Pig Homepage</a>
(contains source code and setup instructions)</p>
<p><a href="http://zookeeper.wiki.sourceforge.net/ZooKeeperGuide">ZooKeeper
Guide</a> - The ZooKeeper manual</p>
<p><a href="http://hadoop.apache.org/zookeeper/">ZooKeeper Homepage</a></p>
<p><a href="http://wiki.apache.org/hadoop/ZooKeeper">ZooKeeper Wiki</a></p>

<div style="text-align: center;">
<p> <a href="module5.html">Previous module</a> &nbsp;|&nbsp; <a href="index.html">Table of contents</a> &nbsp;|&nbsp; <a href="module7.html">Next module</a> </p>

</div>
                       </div>
                        </div>
                        <div class="yui-b" id="ydnRack">
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>
</div>
<div id="clearfooter" style="padding-top: 40px;">
<!-- used to make room for the #footer -->
</div>

</body></html>
